apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: {{ template "etcd.fullname" . }}
  labels:
    heritage: {{ .Release.Service | quote }}
    release: {{ .Release.Name | quote }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    component: "{{ .Release.Name }}-{{ .Values.Component }}"
spec:
  selector:
    matchLabels:
      heritage: {{ .Release.Service | quote }}
      release: {{ .Release.Name | quote }}
      chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
      component: "{{ .Release.Name }}-{{ .Values.Component }}"
  serviceName: {{ .Values.Name }}
  replicas: {{ default 3 .Values.Replicas }}
  template:
    metadata:
      name: {{ template "etcd.fullname" . }}
      labels:
        heritage: {{ .Release.Service | quote }}
        release: {{ .Release.Name | quote }}
        chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
        component: "{{ .Release.Name }}-{{ .Values.Component }}"
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      imagePullSecrets:
      - name: {{ .Values.common.image.pullSecrets }}
      containers:
      - name: {{ .Values.Name }}
        image: "{{ .Values.common.image.repository }}:{{ .Values.common.image.tag }}"
        imagePullPolicy: "{{ .Values.common.image.pullPolicy }}"
        ports:
        - containerPort: {{ .Values.PeerPort }}
          name: peer
        - containerPort: {{ .Values.ClientPort }}
          name: client
        resources:
          requests:
            cpu: "{{ .Values.Cpu }}"
            memory: "{{ .Values.Memory }}"
          # limits:
          #   cpu: "{{ .Values.Cpu }}"
          #  memory: "{{ .Values.Memory }}"
        env:
        - name: INITIAL_CLUSTER_SIZE
          value: {{ default 3 .Values.Replicas | quote }}
        - name: SET_NAME
          value: {{ template "etcd.fullname" . }}
        - name: ETCD_HEARTBEAT_INTERVAL
          value: "600"
        - name: ETCD_ELECTION_TIMEOUT
          value: "6000"
        volumeMounts:
        - name: datadir
          mountPath: /var/run/etcd
        lifecycle:
          preStop:
            exec:
              command:
                - "/bin/bash"
                - "-ec"
                - |
                  EPS=""
                  for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                      EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.{{ .Values.Name }}:{{ .Values.ClientPort }}"
                  done

                  HOSTNAME=$(hostname)

                  member_hash() {
                      etcdctl member list | grep http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }} | cut -d':' -f1 | cut -d'[' -f1
                  }

                  echo "Removing ${HOSTNAME} from etcd cluster"

                  ETCDCTL_ENDPOINT=${EPS} etcdctl member remove $(member_hash)
                  if [ $? -eq 0 ]; then
                      # Remove everything otherwise the cluster will no longer scale-up
                      rm -rf /var/run/etcd/*
                  fi
        command:
          - "/bin/bash"
          - "-ec"
          - |
            HOSTNAME=$(hostname)
            set -x
            IP=$(hostname -i)

            # store member id into PVC for later member replacement
            collect_member() {
              until /usr/local/bin/etcdctl member list ; do
                  sleep 1
              done
              curl -s http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.ClientPort }}/v2/members | jq --raw-output ".members[] | select(.name | contains(\"${HOSTNAME}\")) | .id" > /var/run/etcd/member_id
              cat /var/run/etcd/member_id
              
            }

            eps() {
                EPS=""
                for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                    EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.{{ .Values.Name }}:{{ .Values.ClientPort }}"
                done
                echo ${EPS}
            }

            member_hash() {
                etcdctl member list | grep http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }} | cut -d':' -f1 | cut -d'[' -f1
                echo `etcdctl member list`
            }

            # re-joining after failure?
            if [ -e /var/run/etcd/default.etcd ]; then
                echo "Re-joining etcd member"
                member_id=$(cat /var/run/etcd/member_id)

                # re-join member
                ETCDCTL_ENDPOINT=$(eps) etcdctl member update ${member_id} http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }}
                exec etcd --name ${HOSTNAME} \
                    --listen-peer-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }} \
                    --listen-client-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.ClientPort }},http://127.0.0.1:{{ .Values.ClientPort }} \
                    --advertise-client-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.ClientPort }} \
                    --data-dir /var/run/etcd/default.etcd
            fi

            # etcd-SET_ID
            i=$((${#HOSTNAME}-1))
            SET_ID="${HOSTNAME:$i:1}"
            # adding a new member to existing cluster (assuming all initial pods are available)
            if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
                export ETCDCTL_ENDPOINT=$(eps)

                # member already added?
                MEMBER_HASH=$(member_hash)
                if [ -n "${MEMBER_HASH}" ]; then
                    # the member hash exists but for some reason etcd failed
                    # as the datadir has not be created, we can remove the member
                    # and retrieve new hash
                    etcdctl member remove ${MEMBER_HASH}
                fi

                echo "Adding new member"
                etcdctl member add ${HOSTNAME} http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }} | grep "^ETCD_" > /var/run/etcd/new_member_envs

                if [ $? -ne 0 ]; then
                    echo "Exiting"
                    rm -f /var/run/etcd/new_member_envs
                    exit 1
                fi

                cat /var/run/etcd/new_member_envs
                source /var/run/etcd/new_member_envs

                collect_member &

                exec etcd --name ${HOSTNAME} \
                    --listen-peer-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }} \
                    --listen-client-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.ClientPort }},http://127.0.0.1:{{ .Values.ClientPort }} \
                    --advertise-client-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.ClientPort }} \
                    --data-dir /var/run/etcd/default.etcd \
                    --initial-advertise-peer-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }} \
                    --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                    --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
            fi

            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                while true; do
                    echo "Waiting for ${SET_NAME}-${i}.{{ .Values.Name }} to come up"
                    ping -W 1 -c 1 ${SET_NAME}-${i}.{{ .Values.Name }} > /dev/null && break
                    sleep 1s
                done
            done

            PEERS=""

              for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                  PEERS="${PEERS}${PEERS:+,}${SET_NAME}-${i}=http://${SET_NAME}-${i}.{{ .Values.Name }}:{{ .Values.PeerPort }}"
              done


            collect_member &

            echo "Calling exec etcd"
            # join member
            if [ ${INITIAL_CLUSTER_SIZE} -eq 1 ]; then
              exec etcd --name ${HOSTNAME} \
                --initial-cluster-token lps-etcd \
                --initial-cluster-state new \
                --initial-advertise-peer-urls http://0.0.0.0:{{ .Values.PeerPort }} \
                --listen-peer-urls http://0.0.0.0:{{ .Values.PeerPort }} \
                --initial-cluster ${HOSTNAME}=http://0.0.0.0:{{ .Values.PeerPort }} \
                --listen-client-urls http://0.0.0.0:{{ .Values.ClientPort }} \
                --advertise-client-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.ClientPort }},http://localhost:{{ .Values.ClientPort }} \
                --data-dir /var/run/etcd/default.etcd
            else
              exec etcd --name ${HOSTNAME} \
                --initial-advertise-peer-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }} \
                --listen-peer-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.PeerPort }} \
                --listen-client-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.ClientPort }},http://127.0.0.1:{{ .Values.ClientPort }} \
                --advertise-client-urls http://${HOSTNAME}.{{ .Values.Name }}:{{ .Values.ClientPort }} \
                --initial-cluster-token etcd-cluster-1 \
                --initial-cluster ${PEERS} \
                --initial-cluster-state new \
                --data-dir /var/run/etcd/default.etcd
            fi

  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          # upstream recommended max is 700M
          storage: "{{ .Values.Storage }}"
    {{- if .Values.StorageClass }}
    {{- if (eq "-" .Values.StorageClass) }}
      storageClassName: ""
    {{- else }}
      storageClassName: "{{ .Values.StorageClass }}"
    {{- end }}
    {{- end }}
